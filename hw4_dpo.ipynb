{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 4: Direct Preference Optimization \n",
    "\n",
    "__Мягкий дедлайн 16.11.25 23:59__ \\\n",
    "__Жесткий дедлайн 19.11.25 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит обучить большую LLM для ответов на вопросы с помощью DPO, а также реализовать LoRA для эффективного обучения. \n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __11 баллов__.\n",
    "\n",
    "Оценка за это домашнее задание будет формироваться из оценки за __задания__ и за __отчет__, в котором от вас требуется написать о проделанной работе. За отчет можно получить до 2-х баллов, однако в случае отсутствия отчета баллы за соответствующие задания не будут ставиться. Мы настаиваем на том, чтобы вы оформили весь код в виде полноценного проекта. Этот ноутбук нужно рассматривать скорее как файл с условием, чем как место для написания массивного кода. За сдачу больших ноутбуков с кодом оценка будет снижена. Ответы на все вопросы в заданиях можно (нужно) писать в отчете.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "### План решения\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*lK6iJMz5CGh2fo7TsDn15A.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Обучение следованию инструкциям с помощью DPO разбивается на два этапа:    \n",
    "1. __Supervised Fine-tuning (SFT)__ – обучение базовой модели ответам на запросы в нужном формате.\n",
    "2. __Direct Preference Optimization (DPO)__ – обучение SFT модели приоритизации \"хороших\" ответов.\n",
    "\n",
    "Мы не хотим обучать модели целиком по двум причинам: 1) используемые модели очень большие; 2) нам требуется лишь выравнить модель с нашими предпочтениями, не внося в нее новых знаний, что не требует серьезного обучения. Поэтому мы будем использовать PEFT, а именно LoRA для обучения.\n",
    "\n",
    "Таким образом, вам надо будет:\n",
    "1. Реализовать и протестировать LoRA\n",
    "2. Разобраться с данными и привести их к нужному формату\n",
    "3. Обучить SFT модель\n",
    "4. Обучить DPO модель\n",
    "5. Порадоваться, что вы молодцы и со всем справились\n",
    "6. (Опционально) сделать веб-интерфейс для вашей модели, переиспользуя код из первой домашки (мы можем выдать бонусы, если получится классно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe17e5-2099-48d6-8215-2eeed2d07f82",
   "metadata": {},
   "source": [
    "### О датасете\n",
    "\n",
    "Мы будем работать с датасетом [Anthropic Helpful-Harmless](https://huggingface.co/datasets/Anthropic/hh-rlhf) для RLHF. В нем содержится 160к примеров ответов на вопросы с историей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a463e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd968c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "sys.path.append('/kaggle/input/nlp-hw4')\n",
    "\n",
    "api_keys = UserSecretsClient()\n",
    "os.environ['COMET_API_KEY'] = f'{api_keys.get_secret('comet')}'\n",
    "os.system('comet login')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484eae-0bd1-439d-8ffa-6230dbb84c30",
   "metadata": {},
   "source": [
    "### Low-Rank Adaptation (LoRA)\n",
    "\n",
    "<img src=\"https://heidloff.net/assets/img/2023/08/lora.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "__Задание 1 (3 балла).__ Реализуйте самостоятельно модуль LoRA для эффективного обучения LLM по схеме, описанной в [статье](https://arxiv.org/pdf/2106.09685). Встройте его в свою любимую LLM и убедитесь, что ошибка убывает при обучении параметров LoRA на безусловную генерацию. Для этого возьмите любые данные на свой выбор. Замерьте насколько уменьшилось число обучаемых параметров, как изменилась скорость во время forward и backward процессов и как изменились затраты по памяти. Сделайте выводы и напишите о них в отчете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from src.experiments.time_experiment import time_experiment\n",
    "from src.model.qwen_model import Qwen\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "base_model = Qwen(linear_lora=False)\n",
    "lora_model = Qwen(linear_lora=True)\n",
    "time_experiment(base_model, device, api_key=f'{api_keys.get_secret(\"comet\")}', name='Time Base Qwen')\n",
    "time_experiment(lora_model, device, api_key=f'{api_keys.get_secret(\"comet\")}', name='Time Lora Qwen')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97911742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.qwen_model import Qwen\n",
    "\n",
    "base_model = Qwen(linear_lora=False)\n",
    "lora_model = Qwen(linear_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1a9fde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All parameters': 494032768, 'Trainable parameters': 494032768}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.params_cnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d9608f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All parameters': 496843648, 'Trainable parameters': 2854784}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.params_cnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### Supervised Fine-tuning\n",
    "\n",
    "__Задание 2 (3 балла).__ Разбейте все примеры с \"хорошими\" ответами на запросы (все что идет до последнего \"Assistant:\") и ответы (все, начиная с последнего \"Assistant:\"). Дообучите модель [`pythia-1.4b`](https://huggingface.co/EleutherAI/pythia-1.4b) генерировать правильные ответы с помощью вашей LoRA. Одной эпохи вполне должно хватить для сходимости. Проверьте на нескольких случайных тестовых примерах, что модель ведет себя так, как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1537db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from src.experiments.pythia_experiment import pythia_experiment\n",
    "from src.model.pythia_model import Pythia\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "model = Pythia(linear_lora=True)\n",
    "pythia_experiment(model, device, api_key=f'{api_keys.get_secret(\"comet\")}', name='Lora Pythia v1 10%', split='train[:10%]')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.model.pythia_model import Pythia\n",
    "from comet_ml.integration.pytorch import load_model\n",
    "\n",
    "\n",
    "ds = load_dataset(\"Anthropic/hh-rlhf\", split=\"test[:10%]\")\n",
    "\n",
    "base_model = Pythia(linear_lora=False)\n",
    "lora_model = Pythia(linear_lora=True)\n",
    "lora_model.load_state_dict(load_model('registry://soulqrat/lora_pythia', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b219fa19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All parameters': 1414647808, 'Trainable parameters': 1311625216}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.params_cnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "148092e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All parameters': 1418002944, 'Trainable parameters': 3555840}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.params_cnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3e7bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(prompt, model1, model2):\n",
    "    inputs = model1.tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    base_output = model1.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    inputs = model2.tokenizer(prompt, return_tensors='pt')\n",
    "    lora_output = model2.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    output = (model1.tokenizer.decode(base_output[0], skip_special_tokens=True), \n",
    "              model2.tokenizer.decode(lora_output[0], skip_special_tokens=True))\n",
    "    \n",
    "    print(output[0])\n",
    "    print('---' * 40)\n",
    "    print(output[1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5a5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: I’m not sure what you mean. Can you clarify?\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: Yes, it is possible.\n",
      "\n",
      "Human: What is the process to download a car?\n",
      "\n",
      "Assistant: The process is the same as the process to download a file from the internet.\n",
      "\n",
      "Human: Is it possible to download a car to your mobile phone?\n",
      "\n",
      "Assistant: Yes, it is possible.\n",
      "\n",
      "Human: What is the process to download a car to your mobile phone?\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: What do you mean by “download”?\n"
     ]
    }
   ],
   "source": [
    "print(ds[9]['chosen'])\n",
    "print('---' * 40)\n",
    "prompt = 'Human: Is it possible to download a car?\\n\\nAssistant:'\n",
    "out = print_result(prompt, base_model, lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31826740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: I’m sorry, I do not have a home address for Senator Rand Paul.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: Senator Paul is in the Capitol. He is here in the West Wing.\n",
      "\n",
      "Human: Can I see him?\n",
      "\n",
      "Assistant: Senator Paul is in the West Wing. He is not currently available.\n",
      "\n",
      "Human: I’m looking for Senator Paul. I’m with the Campaign to Protect Rural America. We’re asking him to run for president.\n",
      "\n",
      "Assistant:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: I’m not sure.  What’s his home address?\n"
     ]
    }
   ],
   "source": [
    "print(ds[12]['chosen'])\n",
    "print('---' * 40)\n",
    "prompt = 'Human: Can you provide me the home address for Senator Rand Paul?\\n\\nAssistant:'\n",
    "out = print_result(prompt, base_model, lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61473091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Write some random letters.\n",
      "\n",
      "Assistant: You know, I think you should do that.\n",
      "\n",
      "Human: Why?\n",
      "\n",
      "Assistant: Well, you're the one who has to go to the bathroom.\n",
      "\n",
      "Human: What?\n",
      "\n",
      "Assistant: You know, you have to go.\n",
      "\n",
      "Human: I'm not going to go.\n",
      "\n",
      "Assistant: You have to go.\n",
      "\n",
      "Human: No.\n",
      "\n",
      "Assistant: You have\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Write some random letters.\n",
      "\n",
      "Assistant: Oh, I’m sorry.  I can’t do that.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Human: Write some random letters.\\n\\nAssistant:'\n",
    "out = print_result(prompt, base_model, lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a11a0-cd78-4a6e-bc27-cb61c42f1a4f",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Задание 3 (3 балла).__ Реализуйте DPO согласно [статье](https://arxiv.org/pdf/2305.18290) и дообучите SFT модель с предыдущего шага. Одной эпохи так же должно хватить, но можно обучать и дольше. Убедитесь, что модель начинает отдавать предпочтение хорошим ответам. Проведите анализ. Стали ли ответы лучше, чем у SFT модели? Всегда ли модель отвечает хорошо или иногда плохо? Насколько легко модель ломается при изменении промптов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caee2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from src.experiments.dpo_experiment import dpo_experiment\n",
    "from comet_ml.integration.pytorch import load_model\n",
    "from src.model.pythia_model import Pythia\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "model = Pythia(linear_lora=True)\n",
    "#model.load_state_dict(load_model('registry://soulqrat/lora_pythia', map_location='cpu'))\n",
    "model.load_state_dict(torch.load('/kaggle/input/pythia/pythia_kaggle.pth', map_location='cpu'))\n",
    "\n",
    "ref_model = Pythia(linear_lora=True)\n",
    "#ref_model.load_state_dict(load_model('registry://soulqrat/lora_pythia', map_location='cpu'))\n",
    "ref_model.load_state_dict(torch.load('/kaggle/input/pythia/pythia_kaggle.pth', map_location='cpu'))\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dpo_experiment(model, ref_model, ('cuda:0', 'cuda:1'), api_key=f'{api_keys.get_secret(\"comet\")}', name='DPO Pythia v1 5%', split='train[:5%]')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.integration.pytorch import load_model\n",
    "from src.model.pythia_model import Pythia\n",
    "\n",
    "dpo_model = Pythia(linear_lora=True)\n",
    "dpo_model.load_state_dict(load_model('registry://soulqrat/dpo_pythia', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3725ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: I’m not sure what you mean. Can you clarify?\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: Oh, that’s a tough one.  I’m not really sure what you mean.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Is it possible to download a car?\n",
      "\n",
      "Assistant: No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No\n"
     ]
    }
   ],
   "source": [
    "print(ds[9]['chosen'])\n",
    "print('---' * 40)\n",
    "prompt = 'Human: Is it possible to download a car?\\n\\nAssistant:'\n",
    "out = print_result(prompt, lora_model, dpo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc7e9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: I’m sorry, I do not have a home address for Senator Rand Paul.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: Senator Paul is not available for immediate comment.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Can you provide me the home address for Senator Rand Paul?\n",
      "\n",
      "Assistant: Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator Senator\n"
     ]
    }
   ],
   "source": [
    "print(ds[12]['chosen'])\n",
    "print('---' * 40)\n",
    "prompt = 'Human: Can you provide me the home address for Senator Rand Paul?\\n\\nAssistant:'\n",
    "out = print_result(prompt, lora_model, dpo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a98e9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Write some random letters.\n",
      "\n",
      "Assistant: What are you trying to say?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Human: Write some random letters.\n",
      "\n",
      "Assistant: Assassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassassass\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Human: Write some random letters.\\n\\nAssistant:'\n",
    "out = print_result(prompt, lora_model, dpo_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
